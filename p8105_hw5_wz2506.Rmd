---
title: "p8105_hw5_wz2506"
author: "Weiran Zhang"
date: "11/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}

library(readxl)
library(rvest)
library(broom)

```

**Problem 1**

```{r}
library(tidyverse)

set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

iris_with_missing

```

```{r}
##Function replacing missing values
replace_missing = function(x) {
  if (is.numeric(x)) {
    replace(x, is.na(x), mean(x, na.rm = TRUE))
  } else if(is.character(x)){
    replace(x, is.na(x), "virginica")
  }
}

```

```{r}

output = vector("list", length = 5) ##define an output list
for (i in 1:5) {
  output[[i]] = replace_missing(iris_with_missing[[i]])
  } ##for-loop apply the function
output = map(iris_with_missing, replace_missing)
as_tibble(output) ##make the output an an tidied table form

```

**Problem 2**

```{r}

files = as.data.frame(list.files(path = "./data/", pattern = "*.csv"))
colnames(files) = "file_names"

files

```

```{r}

read_file = function(file_names){
  read_csv(paste0("./data/", file_names))
}

output = purrr::map(files$file_names, read_file)

file_nest = 
  files %>%
  mutate(data = output) %>%
  unnest()

file_nest ##dataframe with file names and data in each files

```

```{r}
##Tidy the dataset

file_tidy = file_nest %>%
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "data") %>%
  mutate(files = str_remove(file_names, ".csv"),
         week = str_remove(week, "week_")) %>%
  separate(file_names, into = c("group", "subject_id"), sep = "_") %>%
  mutate(group = recode(group, "con" = "control", "exp" = "experimental")) 

file_tidy

```

```{r}

ggplot(file_tidy, aes(x = week, y = data, color = subject_id, group = subject_id)) +
  geom_point() +
  geom_line() +
  labs(title = "observations on each subject over time",
       x = "week",
       y = "observations") +
  facet_grid(~group)

```

Comment: For control arm spaghetti plot, we can see that the observations are more concentrated in an interval. For experimental arm, we can see that the observations are more spread out; also the observations for experimental arm are relatively higher than in the control arm.

**Problem 3**

```{r}

regression = function(n = 30, beta0 = 2, beta1) {
  df = tibble(
    x = rnorm(30, mean = 0, sd = 1),
    y = beta0 + beta1 * x + rnorm(30, 0, sqrt(50))
  )
  fit = lm(y ~ x, data = df)
  tidy_fit = broom::tidy(fit)
  tibble(
    beta1_hat = tidy_fit$estimate[2],
    p_value = tidy_fit$p.value[2]
  )
}
```

Generate 10000 datasets from the model when $\beta_1$ = 0
```{r}

set.seed(10)
output_b1 = rerun(100, regression(beta1 = 0)) %>%
  bind_rows()

head(output_b1)

```

Generate 10000 datasets from the model for $\beta_1$ = {1,2,3,4,5,6}
```{r}

beta_value = list("beta1_1"  = 1, "beta1_2"  = 2, "beta1_3"  = 3, "beta1_4"  = 4, "beta1_5"  = 5, "beta1_6"  = 6)

output = vector("list", length = 6)

for (i in 1:6) {
  output[[i]] = rerun(10000, regression(beta1 = beta_value[[i]])) %>%
    bind_rows
}

```

```{r}

sim_results = 
  tibble(beta1_value = c(1,2,3,4,5,6)) %>%
  mutate(output_lists = map(.x = beta1_value, ~rerun(10000, regression(beta1 = .x))),
         estimate_dfs = map(output_lists, bind_rows) 
  ) %>%
  select(-output_lists) %>%
  unnest(estimate_dfs)

head(sim_results)
  
```

```{r}

rejectnull_proportion = sim_results %>%
  filter(p_value < 0.05) %>%
  group_by(beta1_value) %>%
  mutate(power = n()/10000)

ggplot(rejectnull_proportion, aes(x = beta1_value, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "power versus true beta_1 value",
       x = "true value of beta_1",
       y = "power")
  
```

Association between effect size and power: Assume the effect size is the difference between true beta_1 value and zero which is the value of true beta_1 value. From the plot, we can see that as the effect size increases, the power also increases. Thus, power and the effect size are positively correlated.

